{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [X] Use the training set to select the best teacher --> no significant improvement\n",
    "\n",
    "- [X] Test when nodes have \"small data\" (reproduce Pablo's results) --> After a while, decentralised learning starts degrading the performance (as in the Pablo's case). \n",
    "\n",
    "- [ ] Why distillation stops learning after the first few rounds\n",
    "\n",
    "- [ ] Add other FedL as benchmarks\n",
    "\n",
    "- [ ] Probabilistic selection of the teacher model\n",
    "\n",
    "- [X] Probabilitic selection between 'follow the best teacher' and 'FedAvg' --> apparent degradation (with p=0.5); \n",
    "\n",
    "- [ ] Probabilistic model fw\n",
    "\n",
    "- [X] Test other graph topologies: done with barabasi and erdos-renyi\n",
    "\n",
    "- [X] Check the \"distance\" between distilled models, federated model and decentralised models\n",
    "\n",
    "- [ ] Test how many neight you need for being resilient from the averaging model disruption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - deterministico : fedavg parte dopo alcune epoche [Running] --> non serve, ora funziona as expected il 'fallback' automatico\n",
    " - probabilistic : fedavg parte con prob. inv. prop al incremento sull'accuratezza\n",
    " - open question: continuare a cambiare politica o rimanere una  volta cambiata? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches\n",
    "\n",
    "- Federated Learing with FedAvg: the system learns a single global model. to be considered as baseline. \n",
    "- Decentralised Learning (DecFL): Each PAIV acts both as client and parameter server of its neghbourhood. Exchange of models is one-hop.\n",
    "- Distillation: \n",
    "    1. Pure Distillation: a nodes uses the neighbours' models as mixed teacher: for one class it uses the best teacher met so far.  \n",
    "    2. Hybrid: Begin with Pure distillation and then it switches to DecFL when the performance on the validation set start degrading, i.e., the difference with respect to the previous result is greater than `1e-2`. The patience (how long to wait before performing the switch after the first sign of degradation) is 1. Motivation: being aggressive to avoid that the degradation process starts spreading over the network. \n",
    "    3. Self distillation (+ Hybrid): as for 1. and 2. but the nodes perform self distillation: instead of using a real teacher, they use a virtual one whose outputs are noisy (noise is my proposal, probably novel) soft labels.   \n",
    "- Oracle: this is a *benchmark* and irrealistic approach: there is only one global teacher trained on the whole dataset. The nodes use it in the standard teacher-student modality. It might represent an upper bound on what a node can achieve with a KD in decentralised and heterogeneous settings using an omniscent teacher.\n",
    "    \n",
    "\n",
    "Note: \n",
    "- local training epoch for both decentralised learning and distillation lasts 30-50 epochs. Less do not provide good results. \n",
    "- all the approaches are tested on the very same data/partitioning/initial hyperparameter settings. \n",
    "### Tested on pure distillation (1.)\n",
    "- comment: Distillation only gets stuck in poor local minima. PAIVS do not improve after few rounds from the beginning. It's like there is not more significant informative content in the models' exchange  . Probably because the the model circulation is \"slow\" (models' exchange between PAIVs is one-hop)\n",
    "\n",
    "### Small scale Test distillation with fallback on decFL when no improvement \n",
    "- reference experiment: `hybrid-dist_dec-mnist-graph10`\n",
    "- Tested tolerance 1e-2 and patience 2 (Distill with fallback)\n",
    "\n",
    "- Comment: In this settings we observe that after 5 communication rounds the paivs behave similarly to to FedAvg and Decentralised both in train and test. As for Decentralised, there are few PAIVS that diverge w.r.t. the rest of the majority. \n",
    "#### Large scale test on 50 nodes. MNIST\n",
    "- reference experiment: `hybrid-dist_dec-mnist-er_graph50` and `hybrid-dist_dec-mnist-ba_graph50`\n",
    "- Tested tolerance 1e-2 and patience 2 (Distill with fallback); 10 epochs of local training (same for all the tested approaches) \n",
    "- On small scale scenario (10 nodes; mnist; connected graph) the performance of Distillation with fallback on local averaging (Dist+DecFL)are satisfacotory. \n",
    "\n",
    "However, on a larger scenario (50 nodes; MNIST; Barabasi-Albert graph) things slightly change. Precisely the size of the local datasets is smaller and given the non-iid distribution following a zipf with parameter 1.6, the size of hte local datasets is far smaller and, moreover, the number of patterns of the less represented classes can be no more that few tens). Therefore, we are facing a case of small local datasets. Differently from the scenarion with 10 nodes, where the size of local datasets is 5x w.r.t. the current one, peforming few local training epochs proved to be detrimental. Instead, increasing the number of local epochs to 50 (we kept the same scale factor of the dataset size reduction, but it's only an arbitrary heuristically chosen number) the performance of both DecFL and Dist-DecFL improves significantly. The latter, in particular, achieves the same performance of FedL  and eventually outperforms it. \n",
    "\n",
    "The interesting behaviour of adopting Distillation in the first training epochs make the Dist-DecFL approach having the same accuracy trend of FedL. However, the *performance increment saturates after few rounds* (**this is something to further investigate**), probably due to the *lack of novel information* present in the neghbourhood (**to be confirmed by experiemnts**). \n",
    "One possibility is that through the Distillation phase, the different nodes' models start getting topologically close, without suffering the disruption that comes when we do the Federated Average. \n",
    "\n",
    "\n",
    "Looking at both the DecFL and Dist-DecFL train, test and accuracy curves is appears clearly when, on average, it occurs the the nodes change learning approach, i.e., from distillation to averaging. It corresponds to the spike in train and test, and to the drop in the accuracy curve. Interestingly looking the the pure DecFL and Dist-DecFL, we can observe the very same behaviour, just shifted thanks to the initial distillation phase. \n",
    "\n",
    "I've been investigating if self-distillation might help with the decentralised training. The idea is substituting the \"real\" teachers currently covered by the peers' models with a virtual ideal teacher whose output contains random noise. Using it during the first local training improves significantly the PAIVs accuracy. \n",
    "\n",
    "The next question is if this self-distillation approach can be used in decentralised settings to make nodes collaborate without exchanging local models, just a summary (a statistic or a distribution) of their output. \n",
    "I tried with (i) first round with self local distillation and then (ii) virtual distillation using the the aggregated peers' outputs computed as:\n",
    "\n",
    "- policy 1:  (sum of the probabilities) / (sum of the cardinality of each category). Preliminary results show a similr behaviour to the one of pure self distillation, i.e., it reaches a plateau after a number of communication rounds.  any improvement w.r.t. the initial self-distillation performance. The models accuracy over the communication rounds look quite stable. \n",
    "\n",
    "- policy 2: weighted average (of policy 1) where the weights for the p-th paiv and the l-th label is the fraction of times the label occurred during the local training. Same behaviour of 'policy 1'\n",
    "\n",
    "- policy 3: weighted avegare noisy teacher (like in the self distillation phase). Results: No improvement\n",
    "\n",
    "- policy 4: cherry picking of teacher's outputs using the min entropy criterium, i.e., for a given label the teacher map is populated the the peer's avg output with min entropy among those received. Results: no improvement (stuck at 75% accuracy)\n",
    "\n",
    "Discussion (personal): apparently the use of the sole peers' outputs has limited informative power. Interestingly, from the accuracy point of view, using self-distillation and 'best-teacher-label' approaches are equivalent. However, using the models as teachers (even cherrypicked) in place of the virtual ones, looks being beneficial for the averaging phase because models start converging in the parameter space. Conversely, avereaging models after few rounds of self ditillation doesn't have the same effect, or worse, the models' parameters tend to diverge. Therefore, in the latter case, averaging models completely destroys their current configuration, as shown by the sheer accuracy drop or the sheer increment of the train and test losses. *Concluding, using only the outputs of models (average or else) as teachers in the decentralised approach does not help to transfer information between nodes*.\n",
    "\n",
    "**All the above discussion is leading me to the idea that teh distillation approches explored so far are too \"isolated\", i.e., there is not enough communication between nodes. Expecially in distillation it apprears of paramount importace using a common information, which can be the same dataset. In our case the common dataset can only be nodes' local dataset. This means that we have to hose the real models for distillation OR we can start with V-KD and then pass to R-KD (or viceversa).** \n",
    "\n",
    "- we need to use the real models. \n",
    "- using virtual models is not enough or it is a too weak signal (probably more research is needed). \n",
    "\n",
    "<!-- CURRENT RUN:\n",
    "- First training: pure local\n",
    "- If not plateau: r-KD if there are teachers or self-KD if not.\n",
    "- If plateau: social diff update. (no avg substitution) -->\n",
    "\n",
    "**CURRENT PLAN**\n",
    "\n",
    "1. usare oracolo senza anchoring (solo KL term) [Chiara]\n",
    "\n",
    "2. se oracle portiamo assenza di anchoring su real-KD [Chiara]\n",
    "\n",
    "3. rifare self-KD+FedAvg dall'inizio [Lorenzo] (no platoon) \n",
    "    Results: same initial drop as in het-decentralised but steeper growth till 96% accuracy\n",
    "\n",
    "**Possible strategies against FedAvg Drop**\n",
    "notation: \n",
    "- $w_l^t$ local model at time t\n",
    "- $\\{w_i^t\\}_{i=1,\\dots,N}$: models received from the netighbourhood. The index $i$ indicates the $i$-th device\n",
    "- $\\bar{w}^t = \\frac{1}{N}\\sum_{i=1}^N w_i^t$: avg model obtained using vanilla FedAvg\n",
    "- $\\Delta w^t = \\|w_l^t - \\bar{w}^t\\|_2$: L2 norm of the distance between local model and the avg model\n",
    "\n",
    "1. Difference-based aggregation: $w_l^{t+1} = w_l^t - \\frac{(w_l^t - \\bar{w}^t)}{\\Delta w^t}$\n",
    "\n",
    "**Comment on the results for policy 1.**: \n",
    "The accuracy drop problem looks mitigated. In the configuration with no-platoon, all the paivs constantly increment their performance at each communication round.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 topologie di rete: BA, RN, Stoch Block Model\n",
    "- altri dataset + altri data: Fashion MNIST; FeMNIST; Synthetic\n",
    "- parametri \n",
    "    - L_CE + alpha* S_KD\n",
    "    - 0.7 + 0.3 rnd (beta)\n",
    "\n",
    "Sensitivity\n",
    "alpha: \n",
    "- 1 (teacher only)\n",
    "- 0.5\n",
    "- 0.05 DONE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "965c51b320705799021371d36fb5e81062a6110e79b9ff3e5c090f6c3b580841"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
